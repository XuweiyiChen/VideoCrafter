{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuweic/miniconda3/envs/videocrafter/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse, os, sys, glob, yaml, math, random\n",
    "import datetime, time\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from collections import OrderedDict\n",
    "from tqdm import trange, tqdm\n",
    "from einops import repeat\n",
    "from einops import rearrange, repeat\n",
    "from functools import partial\n",
    "import torch\n",
    "from pytorch_lightning import seed_everything\n",
    "import ptp_utils\n",
    "from funcs import load_model_checkpoint, load_prompts, load_image_batch, get_filelist, save_videos\n",
    "from funcs import batch_ddim_sampling\n",
    "from utils.utils import instantiate_from_config\n",
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionControl(abc.ABC):\n",
    "    def step_callback(self, x_t):\n",
    "        return x_t\n",
    "\n",
    "    def between_steps(self):\n",
    "        return\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, attn, is_cross: bool, place_in_unet: str):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, context, video_length, place_in_unet: str):\n",
    "        # b, c, h, w \n",
    "        video_length = 16\n",
    "        context = rearrange(context, \"(b f) d c -> b f d c\", f=video_length)\n",
    "        batch_size = context.shape[0]\n",
    "\n",
    "        if batch_size == 2:\n",
    "            # Do classifier-free guidance\n",
    "            hidden_states_uncondition, hidden_states_condition = context.chunk(2)\n",
    "\n",
    "            if self.cur_step <= self.motion_control_step:\n",
    "                hidden_states_motion_uncondition = hidden_states_uncondition[\n",
    "                    1\n",
    "                ].unsqueeze(0)\n",
    "            else:\n",
    "                hidden_states_motion_uncondition = hidden_states_uncondition[\n",
    "                    0\n",
    "                ].unsqueeze(0)\n",
    "\n",
    "            hidden_states_out_uncondition = torch.cat(\n",
    "                [\n",
    "                    hidden_states_motion_uncondition,\n",
    "                    hidden_states_uncondition[1].unsqueeze(0),\n",
    "                ],\n",
    "                dim=0,\n",
    "            )  # Query\n",
    "            hidden_states_sac_in_uncondition = self.forward(\n",
    "                hidden_states_uncondition[0].unsqueeze(0), video_length, place_in_unet\n",
    "            )\n",
    "            hidden_states_sac_out_uncondition = torch.cat(\n",
    "                [\n",
    "                    hidden_states_sac_in_uncondition,\n",
    "                    hidden_states_uncondition[1].unsqueeze(0),\n",
    "                ],\n",
    "                dim=0,\n",
    "            )  # Key & Value\n",
    "\n",
    "            if self.cur_step <= self.motion_control_step:\n",
    "                hidden_states_motion_condition = hidden_states_condition[1].unsqueeze(0)\n",
    "            else:\n",
    "                hidden_states_motion_condition = hidden_states_condition[0].unsqueeze(0)\n",
    "\n",
    "            hidden_states_out_condition = torch.cat(\n",
    "                [\n",
    "                    hidden_states_motion_condition,\n",
    "                    hidden_states_condition[1].unsqueeze(0),\n",
    "                ],\n",
    "                dim=0,\n",
    "            )  # Query\n",
    "            hidden_states_sac_in_condition = self.forward(\n",
    "                hidden_states_condition[0].unsqueeze(0), video_length, place_in_unet\n",
    "            )\n",
    "            hidden_states_sac_out_condition = torch.cat(\n",
    "                [\n",
    "                    hidden_states_sac_in_condition,\n",
    "                    hidden_states_condition[1].unsqueeze(0),\n",
    "                ],\n",
    "                dim=0,\n",
    "            )  # Key & Value\n",
    "\n",
    "            hidden_states_out = torch.cat(\n",
    "                [hidden_states_out_uncondition, hidden_states_out_condition], dim=0\n",
    "            )\n",
    "            hidden_states_sac_out = torch.cat(\n",
    "                [hidden_states_sac_out_uncondition, hidden_states_sac_out_condition],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        elif batch_size == 1:\n",
    "            if self.cur_step <= self.motion_control_step:\n",
    "                hidden_states_motion = context[1].unsqueeze(0)\n",
    "            else:\n",
    "                hidden_states_motion = context[0].unsqueeze(0)\n",
    "\n",
    "            hidden_states_out = torch.cat(\n",
    "                [hidden_states_motion, context[1].unsqueeze(0)], dim=0\n",
    "            )  # Query\n",
    "            hidden_states_sac_in = self.forward(\n",
    "                context[0].unsqueeze(0), video_length, place_in_unet\n",
    "            )\n",
    "            hidden_states_sac_out = torch.cat(\n",
    "                [hidden_states_sac_in, context[1].unsqueeze(0)], dim=0\n",
    "            )  # Key & Value\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Batch size must be 1 or 2\")\n",
    "        \n",
    "        context = rearrange(context, \"b f d c -> (b f) d c\", f=video_length)\n",
    "        hidden_states_out = rearrange(\n",
    "            hidden_states_out, \"b f d c -> (b f) d c\", f=video_length\n",
    "        )\n",
    "        hidden_states_sac_out = rearrange(\n",
    "            hidden_states_sac_out, \"b f d c -> (b f) d c\", f=video_length\n",
    "        )\n",
    "        self.cur_att_layer += 1\n",
    "        if self.cur_att_layer == self.num_att_layers:\n",
    "            self.cur_att_layer = 0\n",
    "            self.cur_step += 1\n",
    "        return hidden_states_out, hidden_states_sac_out, hidden_states_sac_out\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cur_step = 0\n",
    "        self.cur_att_layer = 0\n",
    "        self.num_att_layers = -1\n",
    "        self.motion_control_step = 0\n",
    "\n",
    "\n",
    "class EmptyControl(AttentionControl):\n",
    "    def forward(self, context, video_length, place_in_unet):\n",
    "        return context\n",
    "\n",
    "\n",
    "class FreeSAC(AttentionControl):\n",
    "    def forward(self, context, video_length, place_in_unet):\n",
    "        hidden_states_sac = (\n",
    "            context[:, 0, :, :].unsqueeze(1).repeat(1, video_length, 1, 1)\n",
    "        )\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--seed\", type=int, default=20230211, help=\"seed for seed_everything\")\n",
    "    parser.add_argument(\"--mode\", default=\"base\", type=str, help=\"which kind of inference mode: {'base', 'i2v'}\")\n",
    "    parser.add_argument(\"--ckpt_path\", type=str, default=None, help=\"checkpoint path\")\n",
    "    parser.add_argument(\"--config\", type=str, help=\"config (yaml) path\")\n",
    "    parser.add_argument(\"--prompt_file\", type=str, default=None, help=\"a text file containing many prompts\")\n",
    "    parser.add_argument(\"--savedir\", type=str, default=None, help=\"results saving path\")\n",
    "    parser.add_argument(\"--savefps\", type=str, default=10, help=\"video fps to generate\")\n",
    "    parser.add_argument(\"--n_samples\", type=int, default=1, help=\"num of samples per prompt\",)\n",
    "    parser.add_argument(\"--ddim_steps\", type=int, default=50, help=\"steps of ddim if positive, otherwise use DDPM\",)\n",
    "    parser.add_argument(\"--ddim_eta\", type=float, default=1.0, help=\"eta for ddim sampling (0.0 yields deterministic sampling)\",)\n",
    "    parser.add_argument(\"--bs\", type=int, default=1, help=\"batch size for inference\")\n",
    "    parser.add_argument(\"--height\", type=int, default=512, help=\"image height, in pixel space\")\n",
    "    parser.add_argument(\"--width\", type=int, default=512, help=\"image width, in pixel space\")\n",
    "    parser.add_argument(\"--frames\", type=int, default=-1, help=\"frames num to inference\")\n",
    "    parser.add_argument(\"--fps\", type=int, default=24)\n",
    "    parser.add_argument(\"--unconditional_guidance_scale\", type=float, default=1.0, help=\"prompt classifier-free guidance\")\n",
    "    parser.add_argument(\"--unconditional_guidance_scale_temporal\", type=float, default=None, help=\"temporal consistency guidance\")\n",
    "    ## for conditional i2v only\n",
    "    parser.add_argument(\"--cond_input\", type=str, default=None, help=\"data dir of conditional input\")\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(args, gpu_num, gpu_no, **kwargs):\n",
    "    ## step 1: model config\n",
    "    ## -----------------------------------------------------------------\n",
    "    config = OmegaConf.load(args.config)\n",
    "    #data_config = config.pop(\"data\", OmegaConf.create())\n",
    "    model_config = config.pop(\"model\", OmegaConf.create())\n",
    "    model = instantiate_from_config(model_config)\n",
    "    model = model.cuda(gpu_no)\n",
    "    assert os.path.exists(args.ckpt_path), f\"Error: checkpoint [{args.ckpt_path}] Not Found!\"\n",
    "    model = load_model_checkpoint(model, args.ckpt_path)\n",
    "    model.eval()\n",
    "\n",
    "    ## sample shape\n",
    "    assert (args.height % 16 == 0) and (args.width % 16 == 0), \"Error: image size [h,w] should be multiples of 16!\"\n",
    "    ## latent noise shape\n",
    "    h, w = args.height // 8, args.width // 8\n",
    "    frames = model.temporal_length if args.frames < 0 else args.frames\n",
    "    channels = model.channels\n",
    "    \n",
    "    ## saving folders\n",
    "    os.makedirs(args.savedir, exist_ok=True)\n",
    "\n",
    "    ## step 2: load data\n",
    "    ## -----------------------------------------------------------------\n",
    "    assert os.path.exists(args.prompt_file), \"Error: prompt file NOT Found!\"\n",
    "    prompt_list = load_prompts(args.prompt_file)\n",
    "    num_samples = len(prompt_list)\n",
    "    filename_list = [f\"{id+1:04d}\" for id in range(num_samples)]\n",
    "\n",
    "    samples_split = num_samples // gpu_num\n",
    "    residual_tail = num_samples % gpu_num\n",
    "    print(f'[rank:{gpu_no}] {samples_split}/{num_samples} samples loaded.')\n",
    "    indices = list(range(samples_split*gpu_no, samples_split*(gpu_no+1)))\n",
    "    if gpu_no == 0 and residual_tail != 0:\n",
    "        indices = indices + list(range(num_samples-residual_tail, num_samples))\n",
    "    prompt_list_rank = [prompt_list[i] for i in indices]\n",
    "\n",
    "    ## conditional input\n",
    "    if args.mode == \"i2v\":\n",
    "        ## each video or frames dir per prompt\n",
    "        cond_inputs = get_filelist(args.cond_input, ext='[mpj][pn][4gj]')   # '[mpj][pn][4gj]'\n",
    "        assert len(cond_inputs) == num_samples, f\"Error: conditional input ({len(cond_inputs)}) NOT match prompt ({num_samples})!\"\n",
    "        filename_list = [f\"{os.path.split(cond_inputs[id])[-1][:-4]}\" for id in range(num_samples)]\n",
    "        cond_inputs_rank = [cond_inputs[i] for i in indices]\n",
    "\n",
    "    filename_list_rank = [filename_list[i] for i in indices]\n",
    "\n",
    "    ## step 3: run over samples\n",
    "    ## -----------------------------------------------------------------\n",
    "    start = time.time()\n",
    "    n_rounds = len(prompt_list_rank) // args.bs\n",
    "    n_rounds = n_rounds+1 if len(prompt_list_rank) % args.bs != 0 else n_rounds\n",
    "    for idx in range(0, n_rounds):\n",
    "        print(f'[rank:{gpu_no}] batch-{idx+1} ({args.bs})x{args.n_samples} ...')\n",
    "        idx_s = idx*args.bs\n",
    "        idx_e = min(idx_s+args.bs, len(prompt_list_rank))\n",
    "        batch_size = idx_e - idx_s\n",
    "        filenames = filename_list_rank[idx_s:idx_e]\n",
    "        noise_shape = [batch_size, channels, frames, h, w]\n",
    "        fps = torch.tensor([args.fps]*batch_size).to(model.device).long()\n",
    "\n",
    "        prompts = prompt_list_rank[idx_s:idx_e]\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        #prompts = batch_size * [\"\"]\n",
    "        text_emb = model.get_learned_conditioning(prompts)\n",
    "\n",
    "        if args.mode == 'base':\n",
    "            cond = {\"c_crossattn\": [text_emb], \"fps\": fps}\n",
    "        elif args.mode == 'i2v':\n",
    "            #cond_images = torch.zeros(noise_shape[0],3,224,224).to(model.device)\n",
    "            cond_images = load_image_batch(cond_inputs_rank[idx_s:idx_e], (args.height, args.width))\n",
    "            cond_images = cond_images.to(model.device)\n",
    "            img_emb = model.get_image_embeds(cond_images)\n",
    "            imtext_cond = torch.cat([text_emb, img_emb], dim=1)\n",
    "            cond = {\"c_crossattn\": [imtext_cond], \"fps\": fps}\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        ## inference\n",
    "        motion_control = 1\n",
    "        motion_control_step = motion_control * args.ddim_steps\n",
    "        attn_controller = FreeSAC()\n",
    "        attn_controller.motion_control_step = motion_control_step\n",
    "        ptp_utils.register_attention_control(model, attn_controller)\n",
    "        batch_samples = batch_ddim_sampling(model, cond, noise_shape, args.n_samples, \\\n",
    "                                                args.ddim_steps, args.ddim_eta, args.unconditional_guidance_scale, **kwargs)\n",
    "        ## b,samples,c,t,h,w\n",
    "        save_videos(batch_samples, args.savedir, filenames, fps=args.savefps)\n",
    "\n",
    "    print(f\"Saved in {args.savedir}. Time used: {(time.time() - start):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@CoLVDM Inference: 2024-02-23-04-36-56\n",
      "AE working on z of shape (1, 4, 64, 64) = 16384 dimensions.\n",
      ">>> model checkpoint loaded.\n",
      "[rank:0] 1/1 samples loaded.\n",
      "[rank:0] batch-1 (1)x1 ...\n",
      "DDIM scale True\n",
      "ddim device cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/xuweic/miniconda3/envs/videocrafter/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m rank, gpu_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Run the inference with the manually set arguments\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 86\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(args, gpu_num, gpu_no, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m attn_controller\u001b[38;5;241m.\u001b[39mmotion_control_step \u001b[38;5;241m=\u001b[39m motion_control_step\n\u001b[1;32m     85\u001b[0m ptp_utils\u001b[38;5;241m.\u001b[39mregister_attention_control(model, attn_controller)\n\u001b[0;32m---> 86\u001b[0m batch_samples \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_ddim_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_eta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m## b,samples,c,t,h,w\u001b[39;00m\n\u001b[1;32m     89\u001b[0m save_videos(batch_samples, args\u001b[38;5;241m.\u001b[39msavedir, filenames, fps\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39msavefps)\n",
      "File \u001b[0;32m~/Documents/VideoCrafter/scripts/evaluation/funcs.py:50\u001b[0m, in \u001b[0;36mbatch_ddim_sampling\u001b[0;34m(model, cond, noise_shape, n_samples, ddim_steps, ddim_eta, cfg_scale, temporal_cfg_scale, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ddim_sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_cond\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[0;32m---> 50\u001b[0m     samples, _ \u001b[38;5;241m=\u001b[39m \u001b[43mddim_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mconditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtemporal_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mconditional_guidance_scale_temporal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemporal_cfg_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mx_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_T\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m## reconstruct from latent to pixel space\u001b[39;00m\n\u001b[1;32m     64\u001b[0m batch_images \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode_first_stage_2DAE(samples)\n",
      "File \u001b[0;32m~/miniconda3/envs/videocrafter/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VideoCrafter/scripts/evaluation/../../lvdm/models/samplers/ddim.py:114\u001b[0m, in \u001b[0;36mDDIMSampler.sample\u001b[0;34m(self, S, batch_size, shape, conditioning, callback, normals_sequence, img_callback, quantize_x0, eta, mask, x0, temperature, noise_dropout, score_corrector, corrector_kwargs, verbose, schedule_verbose, x_T, log_every_t, unconditional_guidance_scale, unconditional_conditioning, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     size \u001b[38;5;241m=\u001b[39m (batch_size, C, T, H, W)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# print(f'Data shape for DDIM sampling is {size}, eta {eta}')\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m samples, intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconditioning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mimg_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mquantize_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_x0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mddim_use_original_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_corrector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mx_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_T\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mlog_every_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_conditioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                                            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[0;32m~/miniconda3/envs/videocrafter/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VideoCrafter/scripts/evaluation/../../lvdm/models/samplers/ddim.py:196\u001b[0m, in \u001b[0;36mDDIMSampler.ddim_sampling\u001b[0;34m(self, cond, shape, x_T, ddim_use_original_steps, callback, timesteps, quantize_denoised, mask, x0, img_callback, log_every_t, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning, verbose, cond_tau, target_size, start_timesteps, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     target_size_ \u001b[38;5;241m=\u001b[39m [target_size[\u001b[38;5;241m0\u001b[39m], target_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m8\u001b[39m, target_size[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m8\u001b[39m]\n\u001b[1;32m    191\u001b[0m     img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[1;32m    192\u001b[0m     img,\n\u001b[1;32m    193\u001b[0m     size\u001b[38;5;241m=\u001b[39mtarget_size_,\n\u001b[1;32m    194\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    195\u001b[0m     )\n\u001b[0;32m--> 196\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample_ddim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_original_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_use_original_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mquantize_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_denoised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_corrector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m                          \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m                          \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_conditioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m motion_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_sample_ddim(motion_img, cond, ts, index\u001b[38;5;241m=\u001b[39mindex, use_original_steps\u001b[38;5;241m=\u001b[39mddim_use_original_steps,\n\u001b[1;32m    205\u001b[0m                 quantize_denoised\u001b[38;5;241m=\u001b[39mquantize_denoised, temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    206\u001b[0m                 noise_dropout\u001b[38;5;241m=\u001b[39mnoise_dropout, score_corrector\u001b[38;5;241m=\u001b[39mscore_corrector,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m                 x0\u001b[38;5;241m=\u001b[39mx0,\n\u001b[1;32m    211\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    213\u001b[0m img, pred_x0 \u001b[38;5;241m=\u001b[39m outs\n",
      "File \u001b[0;32m~/miniconda3/envs/videocrafter/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VideoCrafter/scripts/evaluation/../../lvdm/models/samplers/ddim.py:242\u001b[0m, in \u001b[0;36mDDIMSampler.p_sample_ddim\u001b[0;34m(self, x, c, t, index, repeat_noise, use_original_steps, quantize_denoised, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning, uc_type, conditional_guidance_scale_temporal, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m     e_t_uncond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mapply_model(x, t, unconditional_conditioning, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 242\u001b[0m     e_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     e_t_uncond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mapply_model(x, t, unconditional_conditioning, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/VideoCrafter/scripts/evaluation/../../lvdm/models/ddpm3d.py:522\u001b[0m, in \u001b[0;36mLatentDiffusion.apply_model\u001b[0;34m(self, x_noisy, t, cond, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_concat\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_crossattn\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    520\u001b[0m     cond \u001b[38;5;241m=\u001b[39m {key: cond}\n\u001b[0;32m--> 522\u001b[0m x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x_recon, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_recon[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/videocrafter/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/VideoCrafter/scripts/evaluation/../../lvdm/models/ddpm3d.py:712\u001b[0m, in \u001b[0;36mDiffusionWrapper.forward\u001b[0;34m(self, x, t, c_concat, c_crossattn, c_adm, s, mask, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossattn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    711\u001b[0m     cc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(c_crossattn, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 712\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m## it is just right [b,c,t,h,w]: concatenate in channel dim\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     xc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x] \u001b[38;5;241m+\u001b[39m c_concat, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/videocrafter/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/VideoCrafter/scripts/evaluation/../../lvdm/modules/networks/openaimodel3d.py:556\u001b[0m, in \u001b[0;36mUNetModel.forward\u001b[0;34m(self, x, timesteps, context, features_adapter, fps, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m hs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_blocks):\n\u001b[0;32m--> 556\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddition_attention:\n\u001b[1;32m    558\u001b[0m         h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_attn(h, emb, context\u001b[38;5;241m=\u001b[39mcontext, batch_size\u001b[38;5;241m=\u001b[39mb)\n",
      "File \u001b[0;32m~/miniconda3/envs/videocrafter/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/VideoCrafter/scripts/evaluation/../../lvdm/modules/networks/openaimodel3d.py:41\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb, context, batch_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, emb, batch_size)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, SpatialTransformer):\n\u001b[0;32m---> 41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, TemporalTransformer):\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(b f) c h w -> b c f h w\u001b[39m\u001b[38;5;124m'\u001b[39m, b\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/videocrafter/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/VideoCrafter/scripts/evaluation/../../lvdm/modules/attention.py:275\u001b[0m, in \u001b[0;36mSpatialTransformer.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    273\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_in(x)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks):\n\u001b[0;32m--> 275\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_linear:\n\u001b[1;32m    277\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/videocrafter/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/VideoCrafter/scripts/evaluation/ptp_utils.py:32\u001b[0m, in \u001b[0;36mregister_attention_control.<locals>.block_forward.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Preprocess with controller before calling original forward\u001b[39;00m\n\u001b[1;32m     31\u001b[0m norm_hidden_states \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mnorm1(context)\n\u001b[0;32m---> 32\u001b[0m norm_hidden_states, k_input, v_input \u001b[38;5;241m=\u001b[39m \u001b[43mcontroller\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplace_in_unet\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Modify kwargs for attention inputs\u001b[39;00m\n\u001b[1;32m     37\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk_input\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m k_input\n",
      "Cell \u001b[0;32mIn[2], line 82\u001b[0m, in \u001b[0;36mAttentionControl.__call__\u001b[0;34m(self, context, video_length, place_in_unet)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_step \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmotion_control_step:\n\u001b[0;32m---> 82\u001b[0m         hidden_states_motion \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m         hidden_states_motion \u001b[38;5;241m=\u001b[39m context[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(\"@CoLVDM Inference: %s\" % now)\n",
    "args = argparse.Namespace(\n",
    "    seed=123,\n",
    "    mode=\"base\",\n",
    "    ckpt_path='/home/xuweic/Documents/VideoCrafter/checkpoints/base_512_v2/model.ckpt',\n",
    "    config='/home/xuweic/Documents/VideoCrafter/configs/inference_t2v_512_v2.0.yaml',\n",
    "    prompt_file=\"/home/xuweic/Documents/VideoCrafter/prompts/test_prompts.txt\",\n",
    "    savedir=\"/home/xuweic/Documents/VideoCrafter/results/base_512_v2\",\n",
    "    savefps=10,\n",
    "    n_samples=1,\n",
    "    ddim_steps=50,\n",
    "    ddim_eta=1.0,\n",
    "    bs=1,\n",
    "    height=320,\n",
    "    width=512,\n",
    "    frames=-1,\n",
    "    fps=28,\n",
    "    unconditional_guidance_scale=12.0,\n",
    "    unconditional_guidance_scale_temporal=None,\n",
    "    cond_input=None\n",
    ")\n",
    "\n",
    "# Ensure consistent behavior across runs\n",
    "seed_everything(args.seed)\n",
    "\n",
    "# Assuming these are relevant to your distributed setup or other logic\n",
    "rank, gpu_num = 0, 1\n",
    "\n",
    "# Run the inference with the manually set arguments\n",
    "run_inference(args, gpu_num, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videocrafter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
